#!/usr/bin/env python3
"""
vLLM ì„œë²„ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import json
import requests
from typing import List, Dict, Optional
import time
from tqdm import tqdm
# [NOTE, hyunnnchoi, 2025.11.12] tokenizer ì¶”ê°€ for output slicing
from transformers import AutoTokenizer
# [NOTE, hyunnnchoi, 2025.11.12] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
from concurrent.futures import ThreadPoolExecutor, as_completed

# [NOTE, hyunnnchoi, 2025.11.12] vLLM ì„œë²„ ì„¤ì •
VLLM_SERVER_URL = "http://localhost:8000/v1/completions"  # vLLM ì„œë²„ URL
# [NOTE, hyunnnchoi, 2025.11.12] ëª¨ë¸ ì´ë¦„ì„ gpt-oss-20bë¡œ ë³€ê²½
MODEL_NAME = "gpt-oss-20b"  # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„


def load_prompts(json_file_path: str) -> List[str]:
    """
    JSON íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_file_path: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    """
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    return data.get('prompts', [])


# [NOTE, hyunnnchoi, 2025.11.12] 50í† í°ì”© ëˆ„ì ìœ¼ë¡œ outputì„ ìë¥´ëŠ” í•¨ìˆ˜ ì¶”ê°€
def slice_output_by_tokens(output_text: str, tokenizer, chunk_size: int = 50) -> List[Dict]:
    """
    output í…ìŠ¤íŠ¸ë¥¼ 50í† í°ì”© ëˆ„ì ìœ¼ë¡œ ìë¥´ê³ , ê° ì²­í¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        output_text: ì „ì²´ output í…ìŠ¤íŠ¸
        tokenizer: ì‚¬ìš©í•  tokenizer
        chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 50)
        
    Returns:
        ê° ì²­í¬ì˜ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        [
            {"output_text": "0~50í† í°", "num_tokens": 50, "remaining_tokens": 100},
            {"output_text": "0~100í† í°", "num_tokens": 100, "remaining_tokens": 50},
            ...
        ]
    """
    # output í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
    tokens = tokenizer.encode(output_text, add_special_tokens=False)
    total_tokens = len(tokens)
    
    chunks = []
    current_pos = 0
    
    # 50í† í°ì”© ëˆ„ì ìœ¼ë¡œ ìë¥´ê¸°
    while current_pos < total_tokens:
        next_pos = min(current_pos + chunk_size, total_tokens)
        
        # 0ë¶€í„° next_posê¹Œì§€ì˜ í† í°ì„ ë””ì½”ë”©
        chunk_tokens = tokens[:next_pos]
        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
        
        chunks.append({
            "output_text": chunk_text,
            "num_tokens": next_pos,
            "remaining_tokens": total_tokens - next_pos
        })
        
        current_pos = next_pos
    
    # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì •í™•íˆ total_tokensê°€ ì•„ë‹ˆë©´ ì „ì²´ë¥¼ ì¶”ê°€
    if not chunks or chunks[-1]["num_tokens"] < total_tokens:
        full_text = tokenizer.decode(tokens, skip_special_tokens=True)
        chunks.append({
            "output_text": full_text,
            "num_tokens": total_tokens,
            "remaining_tokens": 0
        })
    
    return chunks


def send_to_vllm(
    prompt: str,
    server_url: str = VLLM_SERVER_URL,
    model_name: str = MODEL_NAME,
    max_tokens: Optional[int] = None,
    temperature: float = 0.7,
    top_p: float = 0.9,
    timeout: int = 300
) -> Optional[Dict]:
    """
    ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ë¥¼ vLLM ì„œë²„ì— ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        prompt: ì „ì†¡í•  í”„ë¡¬í”„íŠ¸
        server_url: vLLM ì„œë²„ URL
        model_name: ëª¨ë¸ ì´ë¦„
        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        temperature: ìƒ˜í”Œë§ ì˜¨ë„
        top_p: nucleus sampling íŒŒë¼ë¯¸í„°
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
    Returns:
        API ì‘ë‹µ ë˜ëŠ” None (ì—ëŸ¬ ë°œìƒ ì‹œ)
    """
    # [NOTE, hyunnnchoi, 2025.11.12] max_tokens ì œí•œ ì œê±° ì˜µì…˜ ì¶”ê°€
    payload = {
        "model": model_name,
        "prompt": prompt,
        "temperature": temperature,
        "top_p": top_p,
    }
    
    # max_tokensê°€ ì§€ì •ëœ ê²½ìš°ë§Œ ì¶”ê°€
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    
    try:
        response = requests.post(
            server_url, 
            json=payload,
            timeout=timeout
        )
        response.raise_for_status()
        return response.json()
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return None


def process_all_prompts(
    json_file_path: str,
    output_file_path: str,
    server_url: str = VLLM_SERVER_URL,
    model_name: str = MODEL_NAME,
    max_tokens: Optional[int] = None,
    temperature: float = 0.7,
    batch_delay: float = 0.0,
    tokenizer_path: str = None,
    batch_size: int = 16
):
    """
    ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        json_file_path: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
        output_file_path: ì¶œë ¥ JSONL íŒŒì¼ ê²½ë¡œ
        server_url: vLLM ì„œë²„ URL
        model_name: ëª¨ë¸ ì´ë¦„
        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        temperature: ìƒ˜í”Œë§ ì˜¨ë„
        batch_delay: ê° ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        tokenizer_path: tokenizer ê²½ë¡œ (Noneì´ë©´ model_name ì‚¬ìš©)
        batch_size: ë™ì‹œì— ì²˜ë¦¬í•  ìš”ì²­ ìˆ˜
    """
    # [NOTE, hyunnnchoi, 2025.11.12] tokenizer ë¡œë“œ
    print(f"ğŸ”§ Tokenizer ë¡œë”© ì¤‘...")
    if tokenizer_path is None:
        tokenizer_path = "/model"  # vLLM ì„œë²„ì˜ ëª¨ë¸ ê²½ë¡œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
        print(f"âœ… Tokenizer ë¡œë“œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âš ï¸ Tokenizer ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ tokenizer ì‚¬ìš©: {e}\n")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")  # fallback
    
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘: {json_file_path}")
    prompts = load_prompts(json_file_path)
    print(f"âœ… {len(prompts)}ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    
    results = []
    training_data = []  # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  í•™ìŠµ ë°ì´í„°
    success_count = 0
    fail_count = 0
    total_training_samples = 0
    
    print(f"ğŸš€ vLLM ì„œë²„ë¡œ ìš”ì²­ ì „ì†¡ ì‹œì‘...")
    print(f"   ì„œë²„ URL: {server_url}")
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   Max tokens: {'ì œí•œ ì—†ìŒ' if max_tokens is None else max_tokens}\n")
    
    # [NOTE, hyunnnchoi, 2025.11.12] ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    def process_single_prompt(idx_prompt_tuple):
        idx, prompt = idx_prompt_tuple
        response = send_to_vllm(
            prompt=prompt,
            server_url=server_url,
            model_name=model_name,
            max_tokens=max_tokens,
            temperature=temperature
        )
        return idx, prompt, response
    
    # [NOTE, hyunnnchoi, 2025.11.12] ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        # ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ì œì¶œ
        futures = {
            executor.submit(process_single_prompt, (idx, prompt)): idx
            for idx, prompt in enumerate(prompts)
        }
        
        # ì™„ë£Œëœ ì‘ì—…ì„ ì²˜ë¦¬
        for future in tqdm(as_completed(futures), total=len(prompts), desc="ì²˜ë¦¬ ì¤‘"):
            try:
                idx, prompt, response = future.result()
                
                # input/outputì„ ëª…í™•í•˜ê²Œ ì €ì¥í•˜ê³  50í† í°ì”© ëˆ„ì ìœ¼ë¡œ ìë¥´ê¸°
                if response:
                    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    output_text = ""
                    if "choices" in response and len(response["choices"]) > 0:
                        output_text = response["choices"][0].get("text", "")
                    
                    # outputì„ 50í† í°ì”© ëˆ„ì ìœ¼ë¡œ ìë¥´ê¸°
                    if output_text.strip():
                        chunks = slice_output_by_tokens(output_text, tokenizer, chunk_size=50)
                        
                        # ê° ì²­í¬ë¥¼ training_dataì— ì¶”ê°€
                        for chunk in chunks:
                            training_entry = {
                                "input_prompt": prompt,
                                "output_prompt": chunk["output_text"],
                                "number_of_output_tokens": chunk["num_tokens"],
                                "remaining_tokens": chunk["remaining_tokens"]
                            }
                            training_data.append(training_entry)
                            total_training_samples += 1
                    
                    result_entry = {
                        "index": idx,
                        "input_prompt": prompt,
                        "output_text": output_text,
                        "full_response": response,
                        "status": "success"
                    }
                    success_count += 1
                else:
                    result_entry = {
                        "index": idx,
                        "input_prompt": prompt,
                        "output_text": "",
                        "error": "Request failed",
                        "status": "failed"
                    }
                    fail_count += 1
                
                results.append(result_entry)
                
            except Exception as e:
                print(f"\nâŒ ì˜ˆì™¸ ë°œìƒ (idx={futures[future]}): {str(e)}")
                result_entry = {
                    "index": futures[future],
                    "input_prompt": prompts[futures[future]],
                    "output_text": "",
                    "error": str(e),
                    "status": "failed"
                }
                results.append(result_entry)
                fail_count += 1
    
    # [NOTE, hyunnnchoi, 2025.11.12] JSONL í˜•ì‹ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì €ì¥
    training_output_path = output_file_path.replace('.json', '_training.jsonl')
    print(f"\nğŸ’¾ í•™ìŠµ ë°ì´í„° ì €ì¥ ì¤‘: {training_output_path}")
    with open(training_output_path, 'w', encoding='utf-8') as f:
        for entry in training_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    # ì›ë³¸ ê²°ê³¼ë„ JSONìœ¼ë¡œ ì €ì¥ (ë””ë²„ê¹…ìš©)
    print(f"ğŸ’¾ ì›ë³¸ ê²°ê³¼ ì €ì¥ ì¤‘: {output_file_path}")
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump({
            "total": len(prompts),
            "success": success_count,
            "failed": fail_count,
            "total_training_samples": total_training_samples,
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì™„ë£Œ!")
    print(f"   ì´ í”„ë¡¬í”„íŠ¸: {len(prompts)}ê°œ")
    print(f"   ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}")
    print(f"   í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {total_training_samples}ê°œ")
    print(f"   í•™ìŠµ ë°ì´í„° íŒŒì¼: {training_output_path}")
    print(f"   ì›ë³¸ ê²°ê³¼ íŒŒì¼: {output_file_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="vLLM ì„œë²„ì— í”„ë¡¬í”„íŠ¸ ì „ì†¡")
    parser.add_argument(
        "--input",
        type=str,
        default="/data/processed_dataset.json",
        help="ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="/data/vllm_results.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--server-url",
        type=str,
        default=VLLM_SERVER_URL,
        help="vLLM ì„œë²„ URL"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=MODEL_NAME,
        help="ëª¨ë¸ ì´ë¦„"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=None,
        help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: None, ì œí•œ ì—†ìŒ)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ìƒ˜í”Œë§ ì˜¨ë„"
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.0,
        help="ê° ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)"
    )
    # [NOTE, hyunnnchoi, 2025.11.12] tokenizer ê²½ë¡œ ì¸ì ì¶”ê°€
    parser.add_argument(
        "--tokenizer-path",
        type=str,
        default=None,
        help="Tokenizer ê²½ë¡œ (ê¸°ë³¸ê°’: /model)"
    )
    # [NOTE, hyunnnchoi, 2025.11.12] ë°°ì¹˜ í¬ê¸° ì¸ì ì¶”ê°€
    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="ë™ì‹œì— ì²˜ë¦¬í•  ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 16)"
    )
    
    args = parser.parse_args()
    
    process_all_prompts(
        json_file_path=args.input,
        output_file_path=args.output,
        server_url=args.server_url,
        model_name=args.model,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
        batch_delay=args.delay,
        tokenizer_path=args.tokenizer_path,
        batch_size=args.batch_size
    )

